<!DOCTYPE html>
<html>
<head>
    <h1>
        Assignment 2
    </h1>
</head>    
    <body>
        Miguel Velasco
ISYS 475
Assignment 2

Apache Spark

Apache Spark is an open source big data processing framework that allows user to enjoy its versatility, fast speed, easiness, and sophisticated analytics. Apache Spark is a unified and comprehensive framework that let users manage and process data, including but not limited to text, numbers, graphs, etc. Developed back in 2009 as a research project at University of California Berkeley, it is built on top of MESOS, an open source cluster management previously developed by the Apache Software Foundation. The main purposes for its development was to create a management program that supported different kind of cluster computing systems. Hadoop, a data processing technology that has been around for about 10 years, is a program where huge amount of batches of data are processed. It uses MapReduce to perform one computation, but when several computations or algorithms are required, its efficiency drops. On the other hand, Apache Spark is a program of real time streaming data. This program allows developers to create the multi-step computations using Directed Acyclic Graph (DAG) patter, supporting data sharing so the same data can be use simultaneously. This process performed by Apache Spark enable applications in Hadoop to run at least 100 times faster. Apache Spark also works with Cassandra and Apache Hbase. Phyton, Java and some other languages are fully supported by Apache Sparks, as well as SQL queries, graph processing and even machine learning. 
As stated before, Apache Spark was originally developed in 2009 at UC Berkeley, but the project was later donated to the Apache Software Foundation, institution that is currently in charge of the project.  There is also a committee created by people in the technology field such as Databricks, Intel, Yahoo, IBM, and Netflix.
As an open source program, developers around the world enhance this project capabilities. There are a variety of ways on how people can help the organization. For example: Helping other users, testing releases, reviewing changes, documentation, bug reports and of course, coding.
Before any contribution, the governance structure of the project requires contributors to evaluate his/her contribution to be new, actionable, and most important, relevant. If algorithms are going to be contributed to the project, these need to be widely known, used and accepted, highly scalable, have strong documentation and accurate documentation, must be backed up with the developer’s user support and also to have a `@Since` annotation on public classes, methods, and variables. 
There is a code review criteria that needs to be followed in order to contribute to the project. There are two categories which are positive and negative. 
Positive additions to the project include: Fixing the root cause of a bug in existing functionality, adds functionality, fixes common problems of a large amount of users, simple and to the point, maintains or improve consistency across Java, Phyton or Scala, tested, reduce the complexity of lines of code, etc.
Negative additions include: short term fixes for a bug, introduce complex and difficult lines of code to the organization, adds complexity, addition of large dependencies, change versions of existing dependencies, addition of large amount of code, and making a lot of modifications at once. 
There is also a disclaimer about the code of conduct which states that all work submitted has to be his/her own work, not someone else’s, and that the developers has the full right to distribute the material.
There are several ways to contribute to this project. Code can be summited through email for further review. Spark also uses JIRA, a software that track bugs or improvements made; and of course, it uses Github to pull request to manage, review, and merge code from http://github.com/apache/spark repository.

    </body>
    
    
</html>